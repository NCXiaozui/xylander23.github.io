<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="John Doe"><title>神经网络入门 · 理想與少年</title><!-- hexo-inject:begin --><!-- hexo-inject:end --><meta name="description" content="[TOC]
人工神经网络（简称神经网络或者类神经网络）是机器学习领域中重要的一个分支，因为其优越的表现越来越受到大家的重视。在这里，主要解释一下神经网络入门知识，包括：基本概念、神经元、全连接神经网络和反向传播算法。
#神经网络概述
神经网络思想每一个机器学习算法其最终目标都是构造一个最优函数$f("><meta name="keywords" content="CS"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/bootstrap.min.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/style.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div id="stage" class="container"><div class="row"><div id="side-bar" class="col-sm-3 col-xs-12 side-container invisible"><div class="vertical-text site-title"><h3 tabindex="-1" class="site-title-small"><a href="/" class="a-title">與數學和零壹斗智斗勇的理想少年</a></h3><h1 tabindex="-1" class="site-title-large"><a href="/" class="a-title">江右少年</a></h1><!--h6(onclick="triggerSiteNav()") Trigger--></div><br class="visible-lg visible-md visible-sm"><div id="site-nav" class="site-title-links"><ul><li><a href="/">Home</a></li><li><a href="/archives">Archive</a></li><li><a href="/categories">Categories</a></li><li><a href="/tags">Tags</a></li><li class="soc"><a href="https://github.com/xylander23" target="_blank" rel="noopener noreferrer"><i class="fa fa-github">&nbsp;</i></a></li></ul><div class="visible-lg visible-md visible-sm site-nav-footer"><br class="site-nav-footer-br"><footer><p>&copy;&nbsp;2020&nbsp;<a target="_blank" href="http://yoursite.com" rel="noopener noreferrer">John Doe</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div><div id="main-container" class="col-sm-9 col-xs-12 main-container invisible"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post-container"><p class="post-title"><a>神经网络入门</a></p><p class="post-meta"><span class="date meta-item">Posted at&nbsp;2018-04-06</span><span class="meta-item"><i class="fa fa-folder"></i><span>&nbsp;</span><a href="/categories/CS/" title="CS" class="a-tag">CS</a><span>&nbsp;</span></span><span class="meta-item"><i class="fa fa-tag"></i><span>&nbsp;</span><a href="/tags/neural-network/" title="neural network" class="a-tag">neural network</a><span>&nbsp;</span></span></p><p class="post-abstract"><p>[TOC]</p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><p>人工神经网络（简称神经网络或者类神经网络）是机器学习领域中重要的一个分支，因为其优越的表现越来越受到大家的重视。在这里，主要解释一下神经网络入门知识，包括：基本概念、神经元、全连接神经网络和反向传播算法。</p>
<p>#神经网络概述</p>
<h2 id="神经网络思想"><a href="#神经网络思想" class="headerlink" title="神经网络思想"></a>神经网络思想</h2><p>每一个机器学习算法其最终目标都是构造一个最优函数$f(x)$ ,神经网络借鉴生物的神经网络概念，通过模拟生物神经网络中的神经元和神经网络连接结构进而得到最优函数$f(x)$</p>
<h2 id="神经网络基本要素"><a href="#神经网络基本要素" class="headerlink" title="神经网络基本要素"></a>神经网络基本要素</h2><p>之前提到神经网络主要是借鉴生物神经网络，所以神经网络其实就是由神经元按一定方式连接的网络。因此，神经网络有两个基本的要素：<strong>神经元</strong>和<strong>网络结构</strong>。</p>
<h2 id="网络基本概念"><a href="#网络基本概念" class="headerlink" title="网络基本概念"></a>网络基本概念</h2><p><strong>输入层：</strong>网络的第一层，我们一般称为输入层</p>
<p><strong>输出层：</strong>网络的最后一层，输出结果的一层，我们称为输出层</p>
<p><strong>隐藏层：</strong>介于输入层与输出层之间，因为训练数据并没有给这些层中每层所需要的输出，所以这些层被称为隐藏层。</p>
<p><strong>深度:</strong>网络中拥有的层数，如下图有三层，所以该网络深度为3。</p>
<p><strong>宽度:</strong>隐藏层的神经元个数，下图的隐藏层有四个，所以是4。</p>
<p><img src="https://s1.ax1x.com/2018/04/06/CCj4Rf.png" alt="image"></p>
<h1 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h1><p>生物神经网络中，一个神经元通常具有多个<strong>树突</strong>，主要用来接受传入信息；而<strong>轴突</strong>只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做“<strong>突触</strong>”。</p>
<p><img src="https://images2015.cnblogs.com/blog/673793/201512/673793-20151229121248198-818698949.jpg" alt="image"></p>
<p>神经元模型是一个包含输入，输出与计算功能的模型。输入可以类比为生物神经元的树突，而输出可以类比为神经元的生物轴突，计算则可以类比为细胞核。下图是一个简单的神经元示意图：</p>
<p><img src="http://ufldl.stanford.edu/wiki/images/3/3d/SingleNeuron.png" alt="image"></p>
<p>这个神经元是以$x_1,x_2,x_3$以及截距作+1为输入值，其输出为$h_{w,b}(x)$。接下来主要看下神经元内部具体的计算机制。</p>
<p><img src="https://s1.ax1x.com/2018/04/06/CCxkng.png" alt="image"></p>
<p>神经元计算过程可以分为两个部分：<strong>线性计算</strong>和激活函数的<strong>非线性变换</strong>。</p>
<p>上图是具体的神经元具体的计算过程：</p>
<ol>
<li>接受输入$x_1,x_2…x_n$</li>
<li>根据$w_1,w_2…w_n$线性求和$$z=\sum_{i=1}^{n}w_ix_i$$</li>
<li>将线性结果$z$通过激活函数非线性变换</li>
<li>输出结果y <h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2></li>
</ol>
<p>激活函数是神经元重要组成部分，其作用是将之前的线性计算结果作一个非线性变换。这样就可以使神经网络应用到众多非线性模型中。下面列举一些常用的激活函数：</p>
<ol>
<li>Sigmoid：$f(x)=\frac{1}{1+exp(-z)}$</li>
</ol>
<p>Sigmoid函数可以将实数映射到（0，1）之间，可以用来做二分类，在特征相差比较复杂或者相差不是特别大时效果比较好。缺点就是反向传播时就会出现梯度消失的情况，从而不能完成深层网络训练。</p>
<ol>
<li>Tanh(双切正切函数)：$f(z)=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}=2sigmoid(2z)-1$</li>
</ol>
<p>tanh在特征相差明显时效果会很好。</p>
<ol>
<li>ReLU(Rectified Linear Unit)=$max(0,x)$</li>
</ol>
<p>ReLU收敛速度会比sigmoid、tanh快很多，有效缓解了梯度消失，但是可能在训练过程中发生权重无法更新的情况。</p>
<h1 id="全连接神经网络"><a href="#全连接神经网络" class="headerlink" title="全连接神经网络"></a>全连接神经网络</h1><p>神经网络就是许多个单一“神经元”联结在一起，这样，一个“神经元”的输出就可以是另一个“神经元”的输入。<img src="http://ufldl.stanford.edu/wiki/images/9/99/Network331.png" alt="image"></p>
<p>上图是一个简单的神经网络示意图。我们用$n_l$表示网络层数，本例$n_l=3$，我们将第$l$层记为$L_l$,所以输入层是$L_1$，输出层是$L_{nl}$。每个圈内的+1表示偏置项。$a_i^{(l)}$表示第$l$层第$i$单元的激活值。网络中参数为$(W,b)=(W^{(1)},b^{(1)},W^{(2)},b^{(2)})$,其中$W_{ij}^{(l)}$表示第l层第j单元与第l+1层第i单元之间的权重，$b_i^{(l)}$是第l+1层第i单元的偏置项。在给定$W,b$下，我们的神经网络可以按照函数$h_{W,b}(x)$计算输出结果。本例神经网络计算步骤如下：</p>
<p>$a_1^{2}=f(W_{11}^{(1)}x_1+W_{12}^{(1)}x_2+W_{13}^{(1)}x_3+b_1^{(1)})$</p>
<p>$a_2^{2}=f(W_{21}^{(1)}x_1+W_{22}^{(1)}x_2+W_{23}^{(1)}x_3+b_2^{(1)})$</p>
<p>$a_3^{2}=f(W_{31}^{(1)}x_1+W_{32}^{(1)}x_2+W_{33}^{(1)}x_3+b_3^{(1)})$</p>
<p>$h_{W,b}(x)=a_1^{3}=f(W_{11}^{(2)}a_1^{(2)}+W_{12}^{(2)}a_2^{(2)}+W_{13}^{(2)}a_3^{(2)}+b_1^{(2)})$</p>
<p>我们用向量表示：</p>
<p>$z^{(2)}=W^{(1)}x+b^{(1)}$</p>
<p>$a^{2}=f(z^{(2)})$</p>
<p>$z^{(3)}=W^{(2)}a^{(2)}+b^{(2)}$</p>
<p>$h_{W,b}(x)=a^{(3)}=f(z^{(3)})$</p>
<p>我们可以将上式归纳成：</p>
<p>$z^{(l+1)}=W^{(l)}a^{(l)}+b^{(l)}$</p>
<p>$a^{(l+1)}=f(z^{(l+1)})$</p>
<p>在上例中，我们发现计算结果是从上一层向下一层传递的，所以我们称之为<strong>前向传播</strong>。</p>
<h1 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h1><p>神经网络是一个监督学习算法，我们可以利用训练集的信息去训练神经网络。</p>
<p>我们有一个训练集的标签集合${(x^{(1)},y^{(1)},..(x^{(m)},y^{(m)}))}$,可以通过梯度下降法去更新神经网络的参数。</p>
<p>对于每个例子：</p>
<p>$$J(W,b;x,y)=\frac{1}{2}||h_{W,b}(x)-y||^2$$</p>
<p>通过梯度下降法计算参数W和b的负梯度方向，更新参数：</p>
<p>$W_{ij}^{(l)}=W_{ij}^{(l)}-\alpha\frac{\partial}{\partial W_{ij}^{(l)}}J(W,b)$</p>
<p>$b_i^{(l)}=b_i^{(l)}-\alpha \frac{\partial}{\partial b_i^{(l)}}J(W,b)$</p>
<p>其中$\alpha$是学习速率。</p>
<p>但是我们发现如果直接计算神经网络的参数难度很大，所以我们借助<strong>反向传播</strong>算法计算每一层的梯度。</p>
<p>我们可以将上面参数更新公式利用偏导的链式公式展开：</p>
<p>  $W_{ij}^{(l)}=W_{ij}^{(l)}-\alpha\frac{\partial}{\partial W_{ij}^{(l)}}J(W,b)=W_{ij}^{(l)}-\alpha\frac{\partial}{\partial z_i^{n_l}}J(W,b)\frac{z_i^{n_l}}{W_{ij}^{l}}$</p>
<p>$b_i^{(l)}=b_i^{(l)}-\alpha \frac{\partial}{\partial b_i^{(l)}}J(W,b)=b_i^{(l)}-\alpha \frac{\partial}{\partial z_i^{n_l}}J(W,b)\frac{z_i^{n_l}}{b_i^{l}}$</p>
<p>我们可以很容易计算出</p>
<p>$\frac{z_i^{n_l}}{W_{ij}^{l}}=x_{ij}^{(l)}$</p>
<p>$\frac{z_i^{n_l}}{b_i^{l}} = 1$</p>
<p>所以我们最终需要计算的是$\frac{\partial}{\partial z_i^{n_l}}J(W,b)$,我们将这个定义为$\delta_{i}^{(l)}$，我们在计算$\delta_{i}^{(l)}$时，就可以利用反向传播算法。</p>
<p>反向传播算法的思路如下：给定一个样例$(x,y)$，我们首先进行“前向传导”运算，计算出网络中所有的激活值，包括 $h_{wb}(x)$ 的输出值。之后，针对第 $l$ 层的每一个节点$i$，我们计算出其“残差”$\delta_{i}^{(l)}$，该残差表明了该节点对最终输出值的残差产生了多少影响。对于最终的输出节点，我们可以直接算出网络产生的激活值与实际值之间的差距，我们将这个差距定义为 $\delta _{i}^{(n_l)}$（第 <img src="http://ufldl.stanford.edu/wiki/images/math/5/b/7/5b7a0657fdea25f29866c8e1d6e884ac.png" alt="\textstyle n_l"> 层表示输出层）。对于隐藏单元我们如何处理呢？我们将基于节点残差的加权值计算$\delta _{i}^{(l)}$，这些节点以 作$\delta _{i}^{(l)}$为输入。</p>
<ol>
<li><p>输出层的残差计算：</p>
<p>$\delta_i^{(n_l)} = \frac{\partial}{\partial z_i^{(n_l)}}\frac{1}{2}||y-h_{W,b}(x)||^2=\frac{\partial \frac{1}{2}||y-a_i^{(nl)}||^2}{\partial a_i^{(nl)}}\frac{\partial a_i^{(n_l)}}{\partial z_i^{(n_l)}}$</p>
</li>
</ol>
<p>​       $\frac{\partial \frac{1}{2}||y-a_i^{(nl)}||^2}{\partial a_i^{(nl)}}=-(y_i-a_i^{(nl)})$</p>
<p>​       $\frac{\partial a_i^{(nl)}}{\partial z_i^{(n_l)}}=a_i^{n_l}(1-a_i^{n_l})$</p>
<p>​      $\delta_i^{(n_l)}=-(y_i-a_i^{(nl)})a_i^{n_l}(1-a_i^{n_l})$</p>
<p>2.隐藏层残差计算：</p>
<p>$\delta_i^{n_l-1}=\frac{\partial J(W,b;x,y)}{\partial z_i^{n_l-1}}=\frac{\partial}{\partial z_i^{n_l-1}}\frac{1}{2}\sum_{j=1}{S_{n_l}}(y-a_j^{(nl)})^2$</p>
<p>$=\frac{1}{2}\sum_{j=1}^{S_{n_l}}\frac{\partial }{\partial z_i^{z_l-1}}(y_j-f(z_j^{(nl)}))^2$ </p>
<p>$=\sum_{j=1}^{S_{n_l}}-(y_j-f(z_j^{(nl)}))\frac{\partial }{\partial z_i^{z_l-1}}f(z_j^{(n_l)})$</p>
<p>$=\sum_{j=1}^{S_{n_l}}-(y_i-f(z_j^{(n_l)}))f^{‘}(z_j^{(n_l)})\frac{\partial z_j^{(n_l)}}{\partial z_i^{(n_l-1)}}$</p>
<p>$=\sum_{j=1}^{S_{n_l}}\delta_j^{(n_l)}\frac{\partial z_j^{(n_l)}}{\partial z_i^{(n_l-1)}}$</p>
<p>$=\sum_{j=1}^{S_{n_l}}\delta_j^{(n_l)}\frac{\partial \sum_{k=1}^{S_{n_l}-1}f(z_k^{(n_l-1)}W_{jk}^{n_l-1})}{\partial z_i^{(n_l-1)}}$</p>
<p>$=a_i^{(n_l-1)}(1-a_i^{(n_l-1)})(\sum_{j=1}^{S_{n_l}}W_{ji}^{n_l-1}\delta_j^{(n_l)})$</p>
<p>我们可以将残差写成向量的形式：</p>
<ul>
<li>输出层：</li>
</ul>
<p>​    $\vec\delta^{n_l}=\vec a^{n_l} (1-\vec a^{n_l}) (\vec y - \vec a^{n_l})$</p>
<ul>
<li><p>隐藏层：</p>
<p>$\vec \delta^{n_i}=\vec a_{n_i}(1-\vec a_{n_i})W_{(n_i+1)}^T\vec \delta^{n_i+1}$ </p>
</li>
</ul>
<p>其实我们可以简单总结为反向传播就是利用了求导的链式公式，从输出层作为输入去反向更新参数。</p>
<h1 id="实例：识别手写数字"><a href="#实例：识别手写数字" class="headerlink" title="实例：识别手写数字"></a>实例：识别手写数字</h1><h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>识别手写数字</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>Sklearn手写数字集，大小1797</p>
<h2 id="编程思想"><a href="#编程思想" class="headerlink" title="编程思想"></a>编程思想</h2><p>通过向量化编程，构建激活函数类、全连接网络层类和神经网络三个类</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><a href="https://github.com/xylander23/NeuralNetwork" target="_blank" rel="noopener">github</a></p>
</p></div><div class="share"><span>Share</span>&nbsp;<span class="soc"><a href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank" class="fa fa-bookmark"></a></span><span class="soc"><a href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));" class="fa fa-weibo"></a></span><span class="soc"><a href="http://twitter.com/home?status=http://yoursite.com/2018/04/06/神经网络入门/%20理想與少年%20神经网络入门" class="fa fa-twitter"></a></span></div><div class="pagination"><p class="clearfix"><span class="pre pagbuttons"><a role="navigation" href="/2018/05/21/统计基础/" title="统计基础"><i class="fa fa-angle-double-left"></i>&nbsp;Previous post: 统计基础</a></span><span>&nbsp;</span><span class="next pagbuttons"><a role="navigation" href="/2018/03/24/如何学好数学/" title="如何学好数学">Next post: 如何学好数学&nbsp;<i class="fa fa-angle-double-right"></i></a></span></p></div><div id="lv-container" data-id="city" data-uid="MTAyMC8zNTE2Mi8xMTY5Nw=="><script type="text/javascript">(function (d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') {
        return;
    }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script><noscript> Please activate JavaScript for write a comment in LiveRe</noscript></div></div></div></div><div class="visible-xs site-bottom-footer"><footer><p>&copy;&nbsp;2020&nbsp;<a target="_blank" href="http://yoursite.com" rel="noopener noreferrer">John Doe</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div></div><script src="/js/jquery-3.1.0.min.js"></script><script src="/js/bootstrap.min.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/google-analytics.js"></script><script src="/js/typography.js"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body></html>